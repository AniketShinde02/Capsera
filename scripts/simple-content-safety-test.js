#!/usr/bin/env node

/**
 * Simple Content Safety Test - No TypeScript imports
 * This tests the basic functionality without complex imports
 */

console.log('ğŸ§ª Simple Content Safety System Test...\n');

// Test 1: Basic File Validation Logic
console.log('ğŸ“‹ Test 1: File Validation Logic');
function testFileValidation() {
  const mockFile = {
    size: 5 * 1024 * 1024, // 5MB
    type: 'image/jpeg',
    name: 'test-image.jpg'
  };
  
  // Check file size
  const isValidSize = mockFile.size <= 10 * 1024 * 1024;
  console.log(`âœ… File size validation: ${isValidSize ? 'PASSED' : 'FAILED'}`);
  
  // Check file type
  const isValidType = mockFile.type.startsWith('image/');
  console.log(`âœ… File type validation: ${isValidType ? 'PASSED' : 'FAILED'}`);
  
  // Check file name
  const suspiciousNames = ['nsfw', 'adult', 'porn', 'sex', 'nude', 'naked'];
  const hasSuspiciousName = suspiciousNames.some(name => 
    mockFile.name.toLowerCase().includes(name)
  );
  const isValidName = !hasSuspiciousName;
  console.log(`âœ… File name validation: ${isValidName ? 'PASSED' : 'FAILED'}`);
  
  return isValidSize && isValidType && isValidName;
}

const fileValidationResult = testFileValidation();
console.log(`ğŸ“Š Overall file validation: ${fileValidationResult ? 'PASSED' : 'FAILED'}\n`);

// Test 2: Content Safety Guidelines
console.log('ğŸ“‹ Test 2: Content Safety Guidelines');
const guidelines = [
  "ğŸš« No nudity, sexual content, or adult material",
  "ğŸš« No violence, gore, or graphic content",
  "ğŸš« No hate speech or discriminatory content",
  "ğŸš« No spam, scams, or misleading content",
  "ğŸš« No illegal activities or harmful content",
  "âœ… Keep content family-friendly and appropriate",
  "âœ… Report any violations you encounter",
  "âœ… Help maintain a safe community environment"
];

console.log(`âœ… Guidelines loaded: ${guidelines.length} rules`);
guidelines.forEach((guideline, index) => {
  console.log(`   ${index + 1}. ${guideline}`);
});
console.log('');

// Test 3: Suspicious Content Detection
console.log('ğŸš¨ Test 3: Suspicious Content Detection');
function testSuspiciousContent() {
  const testCases = [
    { name: 'normal-image.jpg', expected: true },
    { name: 'nsfw-content.jpg', expected: false },
    { name: 'adult-photo.png', expected: false },
    { name: 'family-pic.jpg', expected: true },
    { name: 'porn-image.gif', expected: false },
    { name: 'vacation-snapshot.jpg', expected: true }
  ];
  
  let passed = 0;
  testCases.forEach((testCase, index) => {
    const suspiciousNames = ['nsfw', 'adult', 'porn', 'sex', 'nude', 'naked'];
    const hasSuspiciousName = suspiciousNames.some(name => 
      testCase.name.toLowerCase().includes(name)
    );
    const isSafe = !hasSuspiciousName;
    const testPassed = isSafe === testCase.expected;
    
    if (testPassed) passed++;
    
    console.log(`   ${index + 1}. ${testCase.name}: ${isSafe ? 'SAFE' : 'BLOCKED'} (${testPassed ? 'âœ…' : 'âŒ'})`);
  });
  
  console.log(`ğŸ“Š Suspicious content detection: ${passed}/${testCases.length} tests passed\n`);
  return passed === testCases.length;
}

const suspiciousContentResult = testSuspiciousContent();

// Test 4: Risk Level Calculation
console.log('âš ï¸ Test 4: Risk Level Calculation');
function testRiskLevels() {
  const testScores = [
    { adult: 1, violence: 0, racy: 2, medical: 0, spoof: 0, expected: 'low' },
    { adult: 5, violence: 3, racy: 4, medical: 2, spoof: 1, expected: 'medium' },
    { adult: 8, violence: 7, racy: 9, medical: 6, spoof: 5, expected: 'high' }
  ];
  
  let passed = 0;
  testScores.forEach((testCase, index) => {
    const maxScore = Math.max(testCase.adult, testCase.violence, testCase.racy, testCase.medical, testCase.spoof);
    
    let riskLevel = 'low';
    if (maxScore >= 8) riskLevel = 'high';
    else if (maxScore >= 5) riskLevel = 'medium';
    
    const testPassed = riskLevel === testCase.expected;
    if (testPassed) passed++;
    
    console.log(`   ${index + 1}. Max score: ${maxScore}, Risk: ${riskLevel.toUpperCase()} (${testPassed ? 'âœ…' : 'âŒ'})`);
  });
  
  console.log(`ğŸ“Š Risk level calculation: ${passed}/${testScores.length} tests passed\n`);
  return passed === testScores.length;
}

const riskLevelResult = testRiskLevels();

// Final Results
console.log('ğŸ‰ Test Results Summary:');
console.log('========================');
console.log(`ğŸ“‹ File Validation: ${fileValidationResult ? 'âœ… PASSED' : 'âŒ FAILED'}`);
console.log(`ğŸ“‹ Guidelines: âœ… PASSED (${guidelines.length} rules loaded)`);
console.log(`ğŸš¨ Suspicious Content: ${suspiciousContentResult ? 'âœ… PASSED' : 'âŒ FAILED'}`);
console.log(`âš ï¸ Risk Levels: ${riskLevelResult ? 'âœ… PASSED' : 'âŒ FAILED'}`);

const overallResult = fileValidationResult && suspiciousContentResult && riskLevelResult;
console.log(`\nğŸ† Overall Result: ${overallResult ? 'âœ… ALL TESTS PASSED' : 'âŒ SOME TESTS FAILED'}`);

if (overallResult) {
  console.log('\nğŸš€ Content safety system logic is working correctly!');
  console.log('ğŸ’¡ Note: This test validates the logic, not the actual API integration.');
} else {
  console.log('\nğŸ”§ Some tests failed. Check the implementation for issues.');
}

console.log('\nğŸ“ Next Steps:');
console.log('   1. Fix any failed tests above');
console.log('   2. Test with actual images in the web app');
console.log('   3. Verify Gemini API integration works');
console.log('   4. Test user reporting functionality');

